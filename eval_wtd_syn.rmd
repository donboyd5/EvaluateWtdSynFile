---
title: "Evaluation of weighted synthetic file"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r runall, eval=FALSE, echo=FALSE}
# UPDATE THESE DIRECTORIES WHEN RUN ON A DIFFERENT SYSTEM ----
# ALSO UPDATE DIRECTORIES IN THE system_specific CHUNK BELOW ----
# use this to create a dated output file - just run the lines below manually

# the directory names must be hard-coded here - can't use getwd()

# NOTE: To run the Tax-Calculator part of the program, set eval=TRUE in header of each relevant code chunk
# This will add several minutes to run time.

indir <- "D:/Dropbox/RPrograms PC/OSPC/EvaluateWtdSynFile/" # directory for this file
outdir <- "D:/Google Drive/OSPC/synpuf_analyses/" # PUBLIC directory
rmdfn <- paste0(indir, "eval_wtd_syn.rmd")  # input rmarkdown code (this file)
outfn <- paste0(outdir, "eval_excl_taxcalc_", format(Sys.time(), "%Y-%m-%d"), ".html")
rmarkdown::render(rmdfn, output_format="html_document", output_file=outfn)
```


```{r mainSet options, echo=FALSE, cache=FALSE}
options(width=120)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=14, fig.height=10)
# Note: when saving maps (ggsave), width=16, height=9 seems to get rid of white space

```


```{r libraries, include=FALSE}
library("magrittr")
library("plyr") # needed for ldply; must be loaded BEFORE dplyr
library("tidyverse")
options(tibble.print_max = 60, tibble.print_min = 60) # if more than 60 rows, print 60 - enough for states
# ggplot2 tibble tidyr readr purrr dplyr stringr forcats
library("scales")

library("btools")

library("knitr")

```


```{r system_specific, include=FALSE}
# UPDATE THESE DIRECTORIES ALSO WHEN RUN ON A DIFFERENT SYSTEM ----
# ALSO UPDATE indir and outdir IN THE runall SECTION ABOVE ----
# THOSE CHANGES MUST BE MADE IN THAT SECTION. indir and outdir CANNOT BE MOVED TO HERE!

pufdir <- "D:/Dropbox/OSPC - Shared/IRS_pubuse_2011/" # location of puf2011.csv
synd <- "D:/Google Drive/synpuf/"

tc.cli <- "C:/ProgramData/Anaconda3/Scripts/tc" # location of Tax-Calculator command-line interface

# private directory for Tax-Calculator record-level output that we don't want moved from this machine
tc.dir <- "D:/tcdir/"

```



```{r globals, include=FALSE}
# variables that I believe Tax-Calculator expects to be upper case
# note that FLPDYR and RECID are not synthesized
upper_case_vars <- c("RECID", "MARS", "XTOT", "DSI", "EIC", "FLPDYR", "MIDR") 

# synvars.cats <- c("dsi", "eic", "f2441", "f6251", "fded", "flpdyr", "mars", "midr", "n24", 
#                  "recid", "s006", "xtot") # generally (but not entirely) categorical-type variables
# 
# synvars.vals <- c(
#              "e00200", "e00300", "e00400", "e00600", "e00650", "e00700", "e00800", "e00900", 
#              "e01100", "e01200", "e01400", "e01500", "e01700", 
#              "e02000", "e02100", "e02300", "e02400", 
#              "e03150", "e03210", "e03220", "e03230", "e03240", "e03270", "e03290", "e03300", "e03400", "e03500", 
#              "e07240", "e07260", "e07300", "e07400", "e07600", 
#              "e09700", "e09800", "e09900", 
#              "e11200", "e17500", "e18400", "e18500", "e19200", "e19800", 
#              "e20100", "e20400", "e24515", "e24518", "e26270", "e27200", 
#              "e32800", "e58990", "e62900", "e87521", "e87530", 
#              "p08000", "p22250", "p23250", "p86421")
# 
# excludes <- c("recid", "flpdyr", "p86421") # fded is not in the puf
# 
# synvars <- setdiff(c(synvars.cats, synvars.vals), excludes)
# synvars

agibrks <- c(-Inf, 0, 25e3, 50e3, 75e3, 100e3, 200e3, 500e3, Inf)
```


```{r functions, include=FALSE}
fget <- function(fvec, ftype){
  read_csv(fvec[ftype], 
           col_types = cols(.default= col_double()), 
           n_max=-1) %>%
    mutate(ftype=ftype)
}


change_case <- function(oldnames, upper_case_vars=c("MARS", "XTOT", "DSI", "EIC", "MIDR")){
  # convert variable names to lower case, except for those that should
  # remain upper case
  newnames <- oldnames
  lower_case_indexes <- !newnames %in% upper_case_vars
  lower_case_names <- newnames[lower_case_indexes] %>% str_to_lower
  newnames[lower_case_indexes] <-lower_case_names
  return(newnames)
}


cordf <- function(df){
  # get correlations among vars in a df
  # put in long format with 3 columns:
  #   var1  - text variable name
  #   var2  - text variable name
  #   value - correlation between var1 and var2
  # return as a data frame
  cordf <- cor(df, use="complete.obs") %>%
    as_tibble(rownames = "var1") %>%
    gather(var2, value, -var1)
  return(cordf)
}


trimcor <- function(df){
  # trim a long correlations data frame (such as produced by cordf)
  # so that it doesn't include "self-correlations" such as x1 with x1, which are always 1
  # and includes only 1 version of each correlation -- e.g., x1-x2 correlation, but not x2-x1 correlation
  # returns data frame with 3 columns:
  #   ftype - the grouping variable that was used
  #   combo - text variable naming the correlation -- e.g., "x1-x2"
  #   value - the correlation
  df2 <- df %>%
    filter(!var1==var2) %>%
    mutate(combo=ifelse(var1 < var2, paste0(var1, "-", var2), paste0(var2, "-", var1))) %>%
    arrange(combo) %>%
    group_by(combo) %>%
    filter(row_number()==1) %>%
    select(ftype, combo, value)
  return(df2)
}


```


```{r ONETIME_get_full_puf, include=FALSE, eval=FALSE}
# after creating and saving the puf, set eval=FALSE so it is not rerun
puf.fn <- "puf2011.csv"

puf <- read_csv(paste0(pufdir, puf.fn), 
                col_types = cols(.default= col_double()), 
                n_max=-1)
saveRDS(puf, paste0(tc.dir, "puf.rds"))

```


```{r info, include=FALSE, eval=FALSE}
# #	File	Training share	Method	Seed columns	Runtime (min)
# 1	synpuf1.csv	10%	  RF	DSI, XTOT	8
# 2	synpuf2.csv	100%	RF	DSI, XTOT	55
# 3	synpuf3.csv	10%	  RF	DSI, XTOT, S006	5
# 4	synpuf4.csv	100%	RF	DSI, XTOT, S006	51

```

```{r get_vnames, include=FALSE}
vnames <- read_csv("./data/pufvars.csv")
# vnames

```



```{r define_files, include=FALSE}
# define files to use in the analysis -- true puf, sample, etc.


ttfnames <- c("puf_10p_sample_train.csv", "puf_10p_sample_test.csv")
names(ttfnames) <- c("train", "test")

synfnames <- paste0("synpuf", 1:4, ".csv")
names(synfnames) <- paste0("synpuf", 1:4)

fnames <- paste0(synd, c(ttfnames, synfnames))
names(fnames) <- names(c(ttfnames, synfnames))
# fnames

```


```{r get_runs, include=FALSE}
df <- bind_rows(fget(fnames, "train"), 
                fget(fnames, "test"), 
                fget(fnames, "synpuf1"),
                fget(fnames, "synpuf2"),
                fget(fnames, "synpuf3"),
                fget(fnames, "synpuf4")) %>%
  mutate(MARS=round(MARS))
count(df, ftype)
count(df, MARS)


```


```{r prepare_puf, include=FALSE}
puf <- readRDS(paste0(tc.dir, "puf.rds"))
names(df)
names(puf)

puf.base <- puf %>% 
  filter(!RECID %in% 999996:999999) %>%
  mutate(ftype="puf.full") %>%
  select(one_of(names(df)))

identical(names(df), names(puf.base))

count(df, ftype)

```


```{r create_stack_files, include=FALSE}
# define the desired order of output from the files so we can create a factor to do that
files.order <- c("train", "test", "synpuf1", "synpuf3", "puf.full", "synpuf2", "synpuf4")

stack <- bind_rows(puf.base, df) %>%
  setNames(change_case(names(.))) %>%
  mutate(wt=s006 / 100,
         ftype=factor(ftype, levels=files.order))

evars <- which(str_sub(names(stack), 1, 1)=="e")
pvars <- which(str_sub(names(stack), 1, 1)=="p")
corvars <- c(names(stack)[union(evars, pvars)], "wt")

# count(stack %>% filter(ftype=="puf.full"), ftype, MARS)
# non-integer MARS: synpufx
# good: train, test, puf.full


```


# Initial simple checks
## Sum of weights and wages
```{r sumtabs, echo=TRUE, tidy=TRUE}

stack %>% 
  group_by(ftype) %>% 
  summarise(n=n(),
            wt=sum(wt),
            e00200=sum(e00200)) %>%
  kable(caption="Unweighted sums",
        digits=c(0), 
        format.args=list(big.mark = ','))

```


## Quantiles for weight
```{r}
stack %>% 
  group_by(ftype) %>% 
  do(qtiledf(.$wt)) %>% 
  kable(caption="Unweighted quantiles of wt",
        digits=c(0, 0, 0, rep(1, 7)), 
        format.args=list(big.mark = ','))
```


```{r eval=FALSE}
glimpse(stack)
stack %>%
  filter(ftype %in% c("puf.full", "synpuf4")) %>%
  group_by(ftype, DSI, XTOT) %>%
  summarise(wt=sum(wt / 1e6)) %>%
  spread(ftype, wt) %>%
  janitor::adorn_totals("row") %>%
  kable(digits=3, caption="Sum of weights, millions")

```


## Weighted sums of variables
* Values other than the pdiffs are in $ billions
* diff2 and diff4 are synpuf2 and synpuf4 minus puf.full
* pdiff2 and pdiff4 are diffs as % of puf.full
```{r}
# wtd sums of variables
diffs <- stack %>%
  filter(ftype %in% c("puf.full", "synpuf2", "synpuf4")) %>%
  select(ftype, corvars) %>%
  gather(variable, value, -wt, -ftype) %>%
  group_by(ftype, variable) %>%
  summarise(value=sum(value * wt) / 1e9) %>%
  spread(ftype, value) %>%
  mutate(diff2=synpuf2 - puf.full,
         diff4=synpuf4 - puf.full,
         pdiff2=diff2 / puf.full * 100,
         pdiff4=diff4 / puf.full * 100)  %>%
  left_join(vnames %>% select(variable=vname, vdesc))
# glimpse(diffs)

f <- function(sortvar) {
  diffs %>%
    arrange(-abs(!!sym(sortvar))) %>%
    select(setdiff(names(.), "vdesc"), vdesc) %>%
    kable(digits=1, caption=paste0("Weighted aggregates in $ billions, sorted by abs of: ", sortvar))
}
f("puf.full")
f("diff2")
f("pdiff2")
f("diff4")
f("pdiff4")

```



## How can synpuf2 weight be so far off and yet weighted wages are so close?
```{r}
d <- stack %>%
  filter(ftype %in% c("puf.full", "synpuf2", "synpuf4")) %>%
  mutate(wagegrp=cut(e00200, agibrks, right=FALSE)) %>%
  group_by(ftype, wagegrp) %>%
  summarise(n=n(),
            wt.sum=sum(wt), 
            wagesb=sum(wt * e00200) / 1e9) %>%
  gather(variable, value, -ftype, -wagegrp) %>%
  unite(spreadvar, variable, ftype)  %>%
  spread(spreadvar, value)
names(d)

f <- function(vpart){
  d %>% 
    select(wagegrp, contains(vpart)) %>%
    janitor::adorn_totals("row") %>%
    mutate(diff2=.[[3]] - .[[2]],
           diff4=.[[4]] - .[[2]],
           pdiff2=diff2 / .[[2]] * 100,
           pdiff4=diff4 / .[[2]] * 100) %>%
    kable(caption="Wage-related values by wage group",
          digits=c(0, rep(1, 7)),
          format.args=list(big.mark = ','))
  }

```

### Record count by wage group
```{r}
f("n_")
```


### Weighted records (i.e., returns) by wage group
```{r}
f("wt.sum")
```


### Sum of weighted wages, in $ billions, by wage group
```{r}
f("wagesb")
```


## Unweighted correlations
```{r corrs, echo=TRUE, tidy=TRUE}
# correlations
# names(stack)
cor1 <- stack %>%
  select(ftype, corvars) %>%
  group_by(ftype) %>%
  do(cordf(.[, -1])) %>%
  do(trimcor(.)) %>%
  separate(combo, c("var1", "var2"), remove=FALSE)
# glimpse(cor1)

wtdpuf <- stack %>%
  filter(ftype=="puf.full") %>%
  select(corvars) %>%
  gather(variable, value, -wt) %>%
  group_by(variable) %>%
  summarise(abwtd.sumb=sum(abs(value) * wt) / 1e9) %>%
  arrange(-abwtd.sumb)
# wtdpuf

cor.wtd <- cor1 %>%
  filter(ftype %in% c("puf.full", "synpuf2", "synpuf4")) %>%
  spread(ftype, value) %>%
  mutate(diff2=synpuf2 - puf.full,
         diff4=synpuf4 - puf.full,
         awsumb1=wtdpuf$abwtd.sumb[match(var1, wtdpuf$variable)],
         awsumb2=wtdpuf$abwtd.sumb[match(var2, wtdpuf$variable)]) %>%
  select(-var1, -var2)

cor.wtd %>%
  arrange(-abs(diff4)) %>%
  filter(row_number() <= 50) %>%
  kable(caption="Correlations sorted by abs(diff4): Top 50",
        digits=c(0, rep(3, 5), 1, 1), 
        format.args=list(big.mark = ','))

cor.wtd %>%
  arrange(-abs(awsumb2)) %>%
  filter(row_number() <= 50) %>%
  kable(caption="Correlations sorted by sum(abs(leftmost variable)): Top 50",
        digits=c(0, rep(3, 5), 1, 1), 
        format.args=list(big.mark = ','))

```


# Distributional analysis of weighted variables against "true" puf

## CDF of file weights

```{r}
df <- stack %>%
    filter(ftype %in% c("puf.full", "synpuf2", "synpuf4")) %>%
    select(ftype, value=wt) %>%
    group_by(ftype) %>%
    arrange(value) %>%
    mutate(cum.pct=cumsum(value) / sum(value))

gtitle <- "Cumulative distribution of file weights"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weights"
  
p <- df %>%
  filter(value > .01) %>%
  ggplot(aes(value, cum.pct, colour=ftype, size=ftype)) + 
  geom_line() +
  scale_size_manual(values=c(2, 1, 1)) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))
p

```



## CDFs of weighted values, continuous variables

```{r}

cdfplot <- function(var){
  df <- stack %>%
    filter(ftype %in% c("puf.full", "synpuf2", "synpuf4")) %>%
    select(ftype, wt, value=var) %>%
    group_by(ftype) %>%
    arrange(value) %>%
    mutate(cum.pct=cumsum(wt * value) / sum(wt * value))
  
  # find a good minimum x-axis value to start the plot on -- based on a desired cumulative percentage
  cum.pct.threshold <- .01
  iminval <- min(which(df$cum.pct[df$ftype=="puf.full"] > cum.pct.threshold))
  minval <- df$value[df$ftype=="puf.full"][iminval]
  # minval
  
  capt <- "- x-axis is log10 scale\n- For display purposes x-axis is truncated at left to start at cum.pct=1%"
  gtitle <- paste0("Cumulative distribution of weighted ", var)
  gsub <- "Aggregate records excluded"
  ylab <- paste0("Cumulative proportion of the sum of weighted ", var)
  
  # define x scale break points and associated labels
  sq10 <- c(0, 1e3, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6,
            1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
  xlabs <- scales::comma(sq10 / 1e3)
  xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands"), breaks=sq10, labels=xlabs)
  
  p <- df %>%
    filter(value > minval) %>%
    ggplot(aes(value, cum.pct, colour=ftype)) + 
    geom_line(size=1.5) +
    theme_bw() +
    ggtitle(gtitle, subtitle=gsub) +  labs(caption=capt) +
    scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
    xscale.l10 +
    theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
    theme(plot.caption = element_text(hjust=0, size=rel(.8)))
  return(p)
}

```



```{r}
names(stack)
excl.vars <- c("DSI", "EIC", "fded", "f2441", "f6251", "MARS", "MIDR", "n24", "XTOT", "s006", "ftype", "wt")
cdfvars <- setdiff(names(stack), excl.vars)
cdfvars

for(var in cdfvars) print(cdfplot(var))
# cdfplot("e00200")
# cdfplot("e00300")
# cdfplot("e00600")
# cdfplot("e00900")
# cdfplot("e01500")
# cdfplot("p23250")

```



# Run actual and synthesized data through Tax-Calculator
```{r include=FALSE, eval=FALSE}
# Info on preparing a files for Tax-Calculator
# https://pslmodels.github.io/Tax-Calculator/

# The only required input variables are RECID (a unique filing-unit record identifier) and
# MARS (a positive-valued filing-status indicator). 

# Other variables in the input file must have variable names that are listed in the Input Variables
# section for them to affect the tax calculations. 

# Any variable listed in Input Variables that is not in an input file is automatically set to zero
# for every filing unit. 

# Variables in the input file that are not listed in Input Variables are ignored by Tax-Calculator.

# Tax-Calculator expects that the filing-unit total for each of several earnings-related variables
# is split between the taxpayer and the spouse....do this earnings splitting.

# e00200 = e00200p + e00200s  # wages
# e00900 = e00900p + e00900s  # business income or loss
# e02100 = e02100p + e02100s  # Schedule F net profit/loss

# You will get an error message from Tax-Calculator, and it will stop running, if you do not split the filing-unit
# amount between taxpayer and spouse so that the above equations hold for each filing unit in the input file.

# ...Tax-Calculator expects that the 
# value of ordinary dividends (e00600) will be no less than the value of qualified dividends (e00650) for each
# filing unit.

# And it also expects that the value of total pension and annuity income (e01500) will be no less than
# the value of taxable pension and annuity income (e01700) for each filing unit.


```


```{r functions_for_taxcalc, include=FALSE}
impose_variable_rules <- function(df){
  if("e00650" %in% names(df) & "e00600" %in% names(df)){
    df <- df %>%
      mutate(e00600=pmax(e00650, e00600))
  }
  if("e01500" %in% names(df) & "e01700" %in% names(df)){
    df <- df %>%
      mutate(e01500=pmax(e01500, e01700))
  }
  return(df)
}


splitvars <- function(df){
  # for now, create arbitrary splits of variables that Tax-Calculators requires prime-spouse splits for
  ppct <- ifelse(df$MARS==2, .5, 0)
  df <- df %>%
    mutate(e00200p=e00200 * ppct,
           e00900p=e00900 * ppct,
           e02100p=e02100 * ppct,
           e00200s=e00200 - e00200p,
           e00900s=e00900 - e00900p,
           e02100s=e02100 - e02100p)
  return(df)
  }


tc.wincmd <- function(tc.fn, tc.dir, tc.cli, taxyear=2013){
  # Build a Windows system command that will call the Tax-Calculator CLI. See:
  #   https://pslmodels.github.io/Tax-Calculator/
  # CAUTION: must use full dir names, not relative to working directory
  # 2013 is the FIRST possible tax year that Tax-Calculator will do
  
  tc.infile.fullpath <- shQuote(paste0(paste0(tc.dir, tc.fn)))
  tc.outdir <- shQuote(str_sub(tc.dir, 1, -1)) # must remove trailing "/"

  cmd <- paste0(tc.cli, " ", tc.infile.fullpath, " ", taxyear, " ", "--dump --outdir ", tc.outdir)
  return(cmd)
}

```


```{r prep_file_for_taxcalc, include=FALSE, eval=FALSE}
# files we want to run through tc - will change as new synpuf's are created
# files.order # show the files we can choose from
tc.files <- c("puf.full", "synpuf2", "synpuf4")

tc.base <- stack %>%
  filter(ftype %in% tc.files) %>%
  mutate(RECID=row_number()) %>%
  do(impose_variable_rules(.)) %>%
  do(splitvars(.)) %>%
  select(ftype, wt, RECID, MARS, XTOT, e00200, e00200p, e00200s, e00300, e00900, e00900p, e00900s,
         e00600, e00650, e01500, e01700)

# write tcbase to a file
tc.fn <- "tcbase.csv"
write_csv(tc.base, paste0(tc.dir, tc.fn))

```


```{r run_tc, include=FALSE, eval=FALSE}
cmd <- tc.wincmd(tc.fn, tc.dir, tc.cli)
cmd

a <- proc.time()
system(cmd) # CAUTION: this will overwrite any existing output file that had same input filename!
proc.time() - a # can take a few minutes

```


```{r get_tcresults, include=FALSE, eval=FALSE}
tc.outfn <- "tcbase-13-#-#-#.csv"
tc.output <- read_csv(paste0(tc.dir, tc.outfn), 
                      col_types = cols(.default= col_double()), 
                      n_max=-1)
names(tc.output) %>% sort

# create analysis file with any variables we want from Tax-Calculator
afile <- tc.base %>%
  left_join(tc.output %>% select(RECID, c00100, taxbc))
names(afile) %>% sort

afile %>%
  group_by(ftype) %>%
  summarise_at(vars(c00100, e00200, e00200p, e00650, e00600, XTOT, taxbc), funs(sum))

afile %>%
  group_by(ftype) %>%
  summarise_at(vars(c00100, e00200, e00200p, e00650, e00600, taxbc), funs(sum(. * wt) / 1e9))

```


## Graph cumulative distribution of variables vs. AGI
### Wages
```{r eval=FALSE}
df <- afile %>%
  select(ftype, wt, e00200, c00100) %>%
  group_by(ftype) %>%
  arrange(c00100) %>%
  mutate(cum.pct=cumsum(wt * e00200) / sum(wt * e00200))

capt <- "- x-axis (AGI) is log10 scale\n- For display purposes x-axis is truncated at left to only show AGI >= $10,000"
gtitle <- "Cumulative distribution of weighted wages (e00200) vs. Adjusted gross income"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weighted wages (e00200)"

sq10 <- c(0, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6, 1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
xlabs <- scales::comma(sq10 / 1e3)
xscale.l10 <- scale_x_log10(name="AGI in $ thousands", breaks=sq10, labels=xlabs)

df %>%
  filter(c00100 > 10e3) %>%
  ggplot(aes(c00100, cum.pct, colour=ftype)) + 
  geom_line(size=1.5) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  labs(caption=capt) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  xscale.l10 +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))

```


```{r eval=FALSE}
df <- afile %>%
  select(ftype, wt, taxbc, c00100) %>%
  group_by(ftype) %>%
  arrange(c00100) %>%
  mutate(cum.pct=cumsum(wt * taxbc) / sum(wt * taxbc))

capt <- "- x-axis (AGI) is log10 scale\n- For display purposes x-axis is truncated at left to only show AGI >= $10,000"
gtitle <- "Cumulative distribution of weighted tax before credit (taxbc) vs. Adjusted gross income"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weighted tax before credit (taxbc)"

sq10 <- c(0, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6, 1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
xlabs <- scales::comma(sq10 / 1e3)
xscale.l10 <- scale_x_log10(name="AGI in $ thousands", breaks=sq10, labels=xlabs)

df %>%
  filter(c00100 > 10e3) %>%
  ggplot(aes(c00100, cum.pct, colour=ftype)) + 
  geom_line(size=1.5) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  labs(caption=capt) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  xscale.l10 +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))
```


