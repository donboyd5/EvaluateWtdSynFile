---
title: "Evaluation of weighted synthetic file"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r runall, eval=FALSE, echo=FALSE}
# UPDATE THESE DIRECTORIES WHEN RUN ON A DIFFERENT SYSTEM ----
# ALSO UPDATE DIRECTORIES IN THE system_specific CHUNK BELOW ----
# use this to create a dated output file - just run the lines below manually

# the directory names must be hard-coded here - can't use getwd()

# NOTE: To run the Tax-Calculator part of the program, set eval=TRUE in header of each relevant code chunk
# This will add several minutes to run time.

indir <- "D:/Dropbox/RPrograms PC/OSPC/EvaluateWtdSynFile/" # directory for this file
outdir <- "D:/Google Drive/OSPC/synpuf_analyses/" # PUBLIC directory
rmdfn <- paste0(indir, "eval_wtd_syn.rmd")  # input rmarkdown code (this file)
# outfn <- paste0(outdir, "eval_excl_taxcalc_", format(Sys.time(), "%Y-%m-%d"), ".html")
outfn <- paste0(outdir, "eval_taxcalc_", format(Sys.time(), "%Y-%m-%d-%H:%M"), ".html")
rmarkdown::render(rmdfn, output_format="html_document", output_file=outfn)

```


```{r mainSet options, echo=FALSE, cache=FALSE}
options(width=120)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.width=14, fig.height=10)
# Note: when saving maps (ggsave), width=16, height=9 seems to get rid of white space

```


```{r libraries, include=FALSE}
library("magrittr")
library("plyr") # needed for ldply; must be loaded BEFORE dplyr
library("tidyverse")
options(tibble.print_max = 60, tibble.print_min = 60) # if more than 60 rows, print 60 - enough for states
# ggplot2 tibble tidyr readr purrr dplyr stringr forcats
library("scales")

library("btools") # Anderson, you should not need this. If you do, it's at https://github.com/donboyd5/btools

library("knitr")

```


```{r system_specific, include=FALSE}
# UPDATE THESE DIRECTORIES ALSO WHEN RUN ON A DIFFERENT SYSTEM ----
# ALSO UPDATE indir and outdir IN THE runall SECTION ABOVE ----
# THOSE CHANGES MUST BE MADE IN THAT SECTION. indir and outdir CANNOT BE MOVED TO HERE!

pufdir <- "D:/Dropbox/OSPC - Shared/IRS_pubuse_2011/" # location of puf2011.csv
synd <- "D:/Google Drive/synpuf/"

tc.cli <- "C:/ProgramData/Anaconda3/Scripts/tc" # location of Tax-Calculator command-line interface

# private directory for Tax-Calculator record-level output that we don't want moved from this machine
tc.dir <- "D:/tcdir/"

```



```{r globals, include=FALSE}
# variables that I believe Tax-Calculator expects to be upper case
# note that FLPDYR and RECID are not synthesized
upper_case_vars <- c("RECID", "MARS", "XTOT", "DSI", "EIC", "FLPDYR", "MIDR") 

# synvars.cats <- c("dsi", "eic", "f2441", "f6251", "fded", "flpdyr", "mars", "midr", "n24", 
#                  "recid", "s006", "xtot") # generally (but not entirely) categorical-type variables
# 
# synvars.vals <- c(
#              "e00200", "e00300", "e00400", "e00600", "e00650", "e00700", "e00800", "e00900", 
#              "e01100", "e01200", "e01400", "e01500", "e01700", 
#              "e02000", "e02100", "e02300", "e02400", 
#              "e03150", "e03210", "e03220", "e03230", "e03240", "e03270", "e03290", "e03300", "e03400", "e03500", 
#              "e07240", "e07260", "e07300", "e07400", "e07600", 
#              "e09700", "e09800", "e09900", 
#              "e11200", "e17500", "e18400", "e18500", "e19200", "e19800", 
#              "e20100", "e20400", "e24515", "e24518", "e26270", "e27200", 
#              "e32800", "e58990", "e62900", "e87521", "e87530", 
#              "p08000", "p22250", "p23250", "p86421")
# 
# excludes <- c("recid", "flpdyr", "p86421") # fded is not in the puf
# 
# synvars <- setdiff(c(synvars.cats, synvars.vals), excludes)
# synvars

agibrks <- c(-Inf, 0, 25e3, 50e3, 75e3, 100e3, 200e3, 500e3, Inf)

```


```{r functions, include=FALSE}
fget <- function(ftype, fvec){
  read_csv(fvec[ftype], 
           col_types = cols(.default= col_double()), 
           n_max=-1) %>%
    mutate(ftype=ftype)
}


change_case <- function(oldnames, upper_case_vars=c("MARS", "XTOT", "DSI", "EIC", "MIDR")){
  # convert variable names to lower case, except for those that should
  # remain upper case
  newnames <- oldnames
  lower_case_indexes <- !newnames %in% upper_case_vars
  lower_case_names <- newnames[lower_case_indexes] %>% str_to_lower
  newnames[lower_case_indexes] <-lower_case_names
  return(newnames)
}


cordf <- function(df){
  # get correlations among vars in a df
  # put in long format with 3 columns:
  #   var1  - text variable name
  #   var2  - text variable name
  #   value - correlation between var1 and var2
  # return as a data frame
  cordf <- cor(df, use="pairwise.complete.obs") %>%
    as_tibble(rownames = "var1") %>%
    gather(var2, value, -var1)
  return(cordf)
}


trimcor <- function(df){
  # trim a long correlations data frame (such as produced by cordf)
  # so that it doesn't include "self-correlations" such as x1 with x1, which are always 1
  # and includes only 1 version of each correlation -- e.g., x1-x2 correlation, but not x2-x1 correlation
  # returns data frame with 3 columns:
  #   ftype - the grouping variable that was used
  #   combo - text variable naming the correlation -- e.g., "x1-x2"
  #   value - the correlation
  df2 <- df %>%
    filter(!var1==var2) %>%
    mutate(combo=ifelse(var1 < var2, paste0(var1, "-", var2), paste0(var2, "-", var1))) %>%
    arrange(combo) %>%
    group_by(combo) %>%
    filter(row_number()==1) %>%
    select(ftype, combo, value)
  return(df2)
}


```


```{r ONETIME_get_full_puf, include=FALSE, eval=FALSE}
# after creating and saving the puf, set eval=FALSE so it is not rerun
puf.fn <- "puf2011.csv"

puf <- read_csv(paste0(pufdir, puf.fn), 
                col_types = cols(.default= col_double()), 
                n_max=-1)
saveRDS(puf, paste0(tc.dir, "puf.rds"))

```


```{r info, include=FALSE, eval=FALSE}
# https://docs.google.com/spreadsheets/d/1qTQJd2DGMm5zXnFxyP2Rw-8rszOkLNFrobg-NIikIsw/edit?usp=sharing
# #	File	Training share	Method	Seed columns	Runtime (min)
# 1	synpuf1.csv	10%	  RF	DSI, XTOT	8
# 2	synpuf2.csv	100%	RF	DSI, XTOT	55
# 3	synpuf3.csv	10%	  RF	DSI, XTOT, S006	5
# 4	synpuf4.csv	100%	RF	DSI, XTOT, S006	51
# 5	synpuf5.csv	10%	RF	MARS, DSI, XTOT, S006, F6251, MIDR, FDED	50	6
# 6	synpuf6.csv	100%	RF	MARS, DSI, XTOT, S006, F6251, MIDR, FDED	50	172

# synthpop1.csv

```

```{r get_vnames, include=FALSE}
vnames <- read_csv("./data/pufvars.csv")
# vnames

```



```{r define_files, include=FALSE}
# define vector of names of synthesized files

ttfnames <- c("puf_10p_sample_train.csv", "puf_10p_sample_test.csv")
names(ttfnames) <- c("train", "test")

synfnames <- paste0("synpuf", 1:6, ".csv")
names(synfnames) <- paste0("synpuf", 1:6)

spnums <- 1:3
spnames <- paste0("synthpop", spnums, ".csv")
names(spnames) <- paste0("synthpop", spnums)

fnames <- paste0(synd, c(ttfnames, synfnames, spnames))
names(fnames) <- names(c(ttfnames, synfnames, spnames))
# fnames

```


```{r get_runs, include=FALSE}
names(fnames)

use.files <- c("synpuf6", "synthpop2", "synthpop3")

# df <- ldply(paste0("synpuf", c(4, 6)), fget, fnames)
df <- ldply(use.files, fget, fnames)
glimpse(df)
count(df, ftype)

df <- df %>%
  select(-penratio, -divratio, -E00100, -E09600) %>% # temporary -- remove vars that are not in synpufs
  mutate(MARS=ifelse(ftype %in% names(fnames)[1:6], round(MARS), MARS)) # early files had non-integer MARS

count(df, MARS)

```


```{r prepare_puf, include=FALSE}
puf <- readRDS(paste0(tc.dir, "puf.rds"))
names(df)
names(puf)

puf.base <- puf %>% 
  filter(!RECID %in% 999996:999999) %>%
  mutate(ftype="puf.full") %>%
  select(one_of(names(df)))

identical(names(df), names(puf.base))

```


```{r create_stack_files, include=FALSE}
# define the desired order of output from the files so we can create a factor to do that
# files.order <- c("train", "test", "synpuf1", "synpuf3", "puf.full", "synpuf2", "synpuf4")
# files.order <- c("puf.full", "synpuf4", "synpuf6")
files.order <- c("puf.full", use.files)

stack <- bind_rows(puf.base, df) %>%
  setNames(change_case(names(.))) %>%
  mutate(wt=s006 / 100,
         ftype=factor(ftype, levels=files.order))

# define corvars -- variables for which we want correlations
evars <- which(str_sub(names(stack), 1, 1)=="e") # this will not pick up EIC because that is an upper-case var
pvars <- which(str_sub(names(stack), 1, 1)=="p")
corvars <- c(names(stack)[union(evars, pvars)], "wt") 

# count(stack %>% filter(ftype=="puf.full"), ftype, MARS)
# non-integer MARS: synpufx
# good: train, test, puf.full


```


# Initial simple checks
## Sum of weights and wages
```{r sumtabs, echo=TRUE, tidy=TRUE}

stack %>% 
  group_by(ftype) %>% 
  summarise(n=n(),
            wt=sum(wt),
            e00200=sum(e00200)) %>%
  kable(caption="Unweighted sums",
        digits=c(0), 
        format.args=list(big.mark = ','))

```


## Quantiles for weight
```{r}
stack %>% 
  group_by(ftype) %>% 
  do(qtiledf(.$wt)) %>% 
  kable(caption="Unweighted quantiles of wt",
        digits=c(0, 0, 0, rep(1, 7)), 
        format.args=list(big.mark = ','))
```


## Weighted sums of variables
* Values other than the pdiffs are in $ billions
```{r}
# wtd sums of variables

diff.base <- stack %>%
  select(ftype, corvars) %>%
  gather(variable, value, -wt, -ftype)

db1 <- diff.base %>%
  group_by(ftype, variable) %>%
  summarise(wvalueb=sum(value * wt) / 1e9) %>%
  ungroup

diffs <- left_join(db1 %>% filter(ftype!="puf.full"),
                   db1 %>% filter(ftype=="puf.full") %>% select(variable, wvalueb.puf=wvalueb)) %>%
  select(variable, ftype, wvalueb.puf, wvalueb) %>%
  mutate(diff=wvalueb - wvalueb.puf,
         pdiff=diff / wvalueb.puf * 100) %>%
  left_join(vnames %>% select(variable=vname, vdesc))
glimpse(diffs)

diffs %>%
  arrange(-abs(wvalueb.puf), ftype) %>%
  kable(digits=1, caption="Weighted aggregates in $ billions, sorted by abs puf size then file")

# f <- function(sortvar) {
#   # compare weighted sums, sorted by the sortvar passed to the function
#   diffs %>%
#     arrange(-abs(!!sym(sortvar))) %>%
#     select(setdiff(names(.), "vdesc"), vdesc) %>%
#     kable(digits=1, caption=paste0("Weighted aggregates in $ billions, sorted by abs of: ", sortvar))
# }

# f("puf.full")
# f("synpuf6_diff")
# f("synpuf6_pdiff")

# still needed in synthpop p08000  e09700 e09800 -- all are approx zero

```


## Unweighted correlations
```{r corrs, echo=TRUE, tidy=TRUE}
# correlations
# names(stack)
cor1 <- stack %>%
  select(ftype, corvars) %>%
  group_by(ftype) %>%
  do(cordf(.[, -1])) %>%
  do(trimcor(.)) %>%
  separate(combo, c("var1", "var2"), remove=FALSE) %>%
  ungroup
# glimpse(cor1)

wtdpuf <- stack %>%
  filter(ftype=="puf.full") %>%
  select(corvars) %>%
  gather(variable, value, -wt) %>%
  group_by(variable) %>%
  summarise(abwtd.sumb=sum(abs(value) * wt) / 1e9) %>%
  arrange(-abwtd.sumb)
# wtdpuf

cor.comp <- 
  left_join(cor1 %>% filter(ftype!="puf.full") %>% select(ftype, combo, comp.cor=value),
            cor1 %>% filter(ftype=="puf.full") %>% select(combo, puf.cor=value, var1, var2)) %>%
  left_join(wtdpuf %>% select(var1=variable, wval1=abwtd.sumb)) %>%
  left_join(wtdpuf %>% select(var2=variable, wval2=abwtd.sumb)) %>%
  mutate(sumval=wval1 + wval2)
glimpse(cor.comp)

cor.comp2 <- cor.comp %>%
  mutate(diff=comp.cor - puf.cor) %>%
  select(combo, ftype, puf.cor, comp.cor, diff, sumval)
  
cor.comp2 %>%
  arrange(-sumval, ftype) %>%
  select(-sumval) %>%
  filter(row_number() <= 100) %>%
  kable(caption="Correlations among large variables: Top 100",
        digits=3)

cor.comp2 %>%
  arrange(-abs(diff), ftype) %>%
  select(-sumval) %>%
  filter(row_number() <= 100) %>%
  kable(caption="Correlations: Top 100 worst differences",
        digits=3)

```


# Distributional analysis of weighted variables against "true" puf

## CDF of file weights

```{r eval=FALSE}
dat <- stack %>%
    select(ftype, value=wt) %>%
    group_by(ftype) %>%
    arrange(value) %>%
    mutate(cum.pct=cumsum(value) / sum(value))

gtitle <- "Cumulative distribution of file weights"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weights"
  
p <- dat %>%
  filter(value > .01) %>%
  ggplot(aes(value, cum.pct, colour=ftype, size=ftype)) + 
  geom_line() +
  scale_size_manual(values=c(2, 1, 1)) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))
p

```


```{r}

cdfplot.uw <- function(var){
  print(var)
  vdesc <- vnames$vdesc[match(var, vnames$vname)]
  df <- stack %>%
    dplyr::select(ftype, wt, value=var) %>%
    group_by(ftype) %>%
    arrange(value) %>%
    mutate(cum.pct=cumsum(value) / sum(value))
  
  if(nrow(df) < 100){
    p <- paste0(var, " has only ", nrow(df), " rows.")
    return(p)
  }
  
  # find a good minimum x-axis value to start the plot on -- based on a desired cumulative percentage
  cum.pct.threshold <- .01
  iminval <- min(which(df$cum.pct[df$ftype=="puf.full"] > cum.pct.threshold))
  minval <- df$value[df$ftype=="puf.full"][iminval]
  # minval
  
  capt <- "- x-axis is log10 scale\n- For display purposes x-axis is truncated at left to start at puf.full's cum.pct=1%"
  gtitle <- paste0("Cumulative distribution of UNweighted ", var, ": ", vdesc)
  gsub <- "Aggregate records excluded"
  ylab <- paste0("Cumulative proportion of the sum of UNweighted ", var)
  
  # define x scale break points and associated labels
  sq10 <- c(0, 1e3, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6,
            1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
  xlabs <- scales::comma(sq10 / 1e3)
  xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands"), breaks=sq10, labels=xlabs)
  
  p <- df %>%
    filter(value > minval) %>%
    ggplot(aes(value, cum.pct, colour=ftype)) + 
    geom_line(size=1.5) +
    theme_bw() +
    ggtitle(gtitle, subtitle=gsub) +  labs(caption=capt) +
    scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
    xscale.l10 +
    theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
    theme(plot.caption = element_text(hjust=0, size=rel(.8)))
  return(p)
}

cdfplot <- function(var){
  print(var)
  vdesc <- vnames$vdesc[match(var, vnames$vname)]
  df <- stack %>%
    dplyr::select(ftype, wt, value=var) %>%
    group_by(ftype) %>%
    arrange(value) %>%
    mutate(cum.pct=cumsum(wt * value) / sum(wt * value))
    
  if(nrow(df) < 100){
    p <- paste0(var, " has only ", nrow(df), " rows.")
    return(p)
  }
  
  # find a good minimum x-axis value to start the plot on -- based on a desired cumulative percentage
  cum.pct.threshold <- .01
  iminval <- min(which(df$cum.pct[df$ftype=="puf.full"] > cum.pct.threshold))
  minval <- df$value[df$ftype=="puf.full"][iminval]
  # minval
  
  capt <- "- x-axis is log10 scale\n- For display purposes x-axis is truncated at left to start at puf.full's cum.pct=1%"
  gtitle <- paste0("Cumulative distribution of weighted ", var, ": ", vdesc)
  gsub <- "Aggregate records excluded"
  ylab <- paste0("Cumulative proportion of the sum of weighted ", var)
  
  # define x scale break points and associated labels
  sq10 <- c(0, 1e3, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6,
            1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
  xlabs <- scales::comma(sq10 / 1e3)
  xscale.l10 <- scale_x_log10(name=paste0(var, " in $ thousands"), breaks=sq10, labels=xlabs)
  
  p <- df %>%
    filter(value > minval) %>%
    ggplot(aes(value, cum.pct, colour=ftype)) + 
    geom_line(size=1.5) +
    theme_bw() +
    ggtitle(gtitle, subtitle=gsub) +  labs(caption=capt) +
    scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
    xscale.l10 +
    theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
    theme(plot.caption = element_text(hjust=0, size=rel(.8)))
  return(p)
}


```


```{r}
names(stack)
excl.vars <- c("DSI", "EIC", "fded", "f2441", "f6251", "MARS", "MIDR", "n24", "XTOT", "s006", "ftype", "wt")
cdfvars <- setdiff(names(stack), excl.vars)
cdfvars
```


## CDFs of UNweighted values, continuous variables
```{r}
for(var in cdfvars) print(cdfplot.uw(var))
```



## CDFs of weighted values, continuous variables
```{r}
for(var in cdfvars) print(cdfplot(var))
```



# Run actual and synthesized data through Tax-Calculator
```{r include=FALSE, eval=FALSE}
# Info on preparing a files for Tax-Calculator
# https://pslmodels.github.io/Tax-Calculator/

# The only required input variables are RECID (a unique filing-unit record identifier) and
# MARS (a positive-valued filing-status indicator). 

# Other variables in the input file must have variable names that are listed in the Input Variables
# section for them to affect the tax calculations. 

# Any variable listed in Input Variables that is not in an input file is automatically set to zero
# for every filing unit. 

# Variables in the input file that are not listed in Input Variables are ignored by Tax-Calculator.

# Tax-Calculator expects that the filing-unit total for each of several earnings-related variables
# is split between the taxpayer and the spouse....do this earnings splitting.

# e00200 = e00200p + e00200s  # wages
# e00900 = e00900p + e00900s  # business income or loss
# e02100 = e02100p + e02100s  # Schedule F net profit/loss

# You will get an error message from Tax-Calculator, and it will stop running, if you do not split the filing-unit
# amount between taxpayer and spouse so that the above equations hold for each filing unit in the input file.

# ...Tax-Calculator expects that the 
# value of ordinary dividends (e00600) will be no less than the value of qualified dividends (e00650) for each
# filing unit.

# And it also expects that the value of total pension and annuity income (e01500) will be no less than
# the value of taxable pension and annuity income (e01700) for each filing unit.


```


```{r functions_for_taxcalc, include=FALSE}
impose_variable_rules <- function(df){
  if("e00650" %in% names(df) & "e00600" %in% names(df)){
    df <- df %>%
      mutate(e00600=pmax(e00650, e00600))
  }
  if("e01500" %in% names(df) & "e01700" %in% names(df)){
    df <- df %>%
      mutate(e01500=pmax(e01500, e01700))
  }
  return(df)
}


splitvars <- function(df){
  # for now, create arbitrary splits of variables that Tax-Calculators requires prime-spouse splits for
  ppct <- ifelse(df$MARS==2, .5, 0)
  df <- df %>%
    mutate(e00200p=e00200 * ppct,
           e00900p=e00900 * ppct,
           e02100p=e02100 * ppct,
           e00200s=e00200 - e00200p,
           e00900s=e00900 - e00900p,
           e02100s=e02100 - e02100p)
  return(df)
  }


tc.wincmd <- function(tc.fn, tc.dir, tc.cli, taxyear=2013){
  # Build a Windows system command that will call the Tax-Calculator CLI. See:
  #   https://pslmodels.github.io/Tax-Calculator/
  # CAUTION: must use full dir names, not relative to working directory
  # 2013 is the FIRST possible tax year that Tax-Calculator will do
  
  tc.infile.fullpath <- shQuote(paste0(paste0(tc.dir, tc.fn)))
  tc.outdir <- shQuote(str_sub(tc.dir, 1, -1)) # must remove trailing "/"

  cmd <- paste0(tc.cli, " ", tc.infile.fullpath, " ", taxyear, " ", "--dump --outdir ", tc.outdir)
  return(cmd)
}

```


```{r prep_file_for_taxcalc, include=FALSE, eval=TRUE}
# files we want to run through tc - will change as new synpuf's are created
# files.order # show the files we can choose from
tc.files <- c("puf.full", use.files) # "synpuf4"
names(stack)

navars <- c("DSI", "f6251", "MIDR", "fded", "EIC", "n24", "f2441", "e09800", "e09700", "p08000")
tc.vars <- setdiff(names(stack),navars)

tc.base <- stack %>%
  filter(ftype %in% tc.files) %>%
  select(tc.vars) %>%
  mutate(RECID=row_number()) %>%
  do(impose_variable_rules(.)) %>% # not needed for synpuf5 and later
  do(splitvars(.))
# write tcbase to a file
tc.fn <- "tcbase.csv"
write_csv(tc.base, paste0(tc.dir, tc.fn))

summary(tc.base)

```


```{r run_tc, include=FALSE, eval=TRUE}
cmd <- tc.wincmd(tc.fn, tc.dir, tc.cli)
cmd

a <- proc.time()
system(cmd) # CAUTION: this will overwrite any existing output file that had same input filename!
proc.time() - a # can take a few minutes

```


```{r get_tcresults, include=FALSE, eval=TRUE}
tc.outfn <- "tcbase-13-#-#-#.csv"
tc.output <- read_csv(paste0(tc.dir, tc.outfn), 
                      col_types = cols(.default= col_double()), 
                      n_max=-1)
names(tc.output) %>% sort

# create analysis file with any variables we want from Tax-Calculator
afile <- tc.base %>%
  left_join(tc.output %>% select(RECID, c00100, taxbc))
names(afile) %>% sort

afile %>%
  group_by(ftype) %>%
  summarise_at(vars(c00100, e00200, e00200p, e00650, e00600, XTOT, taxbc), funs(sum))

afile %>%
  group_by(ftype) %>%
  summarise_at(vars(c00100, e00200, e00200p, e00650, e00600, taxbc), funs(sum(. * wt) / 1e9))

```


## Graph cumulative distribution of variables vs. AGI
### Wages
```{r eval=TRUE}
df2 <- afile %>%
  select(ftype, wt, e00200, c00100) %>%
  group_by(ftype) %>%
  arrange(c00100) %>%
  mutate(cum.pct=cumsum(wt * e00200) / sum(wt * e00200))

capt <- "- x-axis (AGI) is log10 scale\n- For display purposes x-axis is truncated at left to only show AGI >= $10,000"
gtitle <- "Cumulative distribution of weighted wages (e00200) vs. Adjusted gross income"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weighted wages (e00200)"

sq10 <- c(0, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6, 1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
xlabs <- scales::comma(sq10 / 1e3)
xscale.l10 <- scale_x_log10(name="AGI in $ thousands", breaks=sq10, labels=xlabs)

df2 %>%
  filter(c00100 > 10e3) %>%
  ggplot(aes(c00100, cum.pct, colour=ftype)) + 
  geom_line(size=1.5) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  labs(caption=capt) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  xscale.l10 +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))

```


### Tax before credit
```{r eval=TRUE}
df2 <- afile %>%
  select(ftype, wt, taxbc, c00100) %>%
  group_by(ftype) %>%
  arrange(c00100) %>%
  mutate(cum.pct=cumsum(wt * taxbc) / sum(wt * taxbc))

capt <- "- x-axis (AGI) is log10 scale\n- For display purposes x-axis is truncated at left to only show AGI >= $10,000"
gtitle <- "Cumulative distribution of weighted tax before credit (taxbc) vs. Adjusted gross income"
gsub <- "Aggregate records excluded"
ylab <- "Cumulative proportion of the sum of weighted tax before credit (taxbc)"

sq10 <- c(0, 10e3, 25e3, 50e3, 100e3, 250e3, 500e3, 750e3, 1e6, 1.5e6, 2e6, 3e6, 4e6, 5e6, 10e6, 25e6, 50e6, 100e6)
xlabs <- scales::comma(sq10 / 1e3)
xscale.l10 <- scale_x_log10(name="AGI in $ thousands", breaks=sq10, labels=xlabs)

df2 %>%
  filter(c00100 > 10e3) %>%
  ggplot(aes(c00100, cum.pct, colour=ftype)) + 
  geom_line(size=1.5) +
  theme_bw() +
  ggtitle(gtitle, subtitle=gsub) +
  labs(caption=capt) +
  scale_y_continuous(name=ylab, breaks=c(seq(0, .9, .05), seq(.92, 1, .02))) +
  xscale.l10 +
  theme(axis.text.x=element_text(angle=45, size=10, hjust=1, colour="black")) +
  theme(plot.caption = element_text(hjust=0, size=rel(.8)))
```


